{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16c9a79",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GETA Compressed Model Extractor\n",
    "\n",
    "This notebook helps extract compressed models from GETA checkpoints. The compression is achieved through the Only-Train-Once (OTO) library which is integrated with the GETA compression technique.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Import required libraries\n",
    "2. Define extraction function\n",
    "3. Extract compressed model from checkpoint\n",
    "4. Analyze and visualize model statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e174eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to sys.path to be able to import OpenGait modules\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd())))\n",
    "\n",
    "# Import OpenGait and Only-Train-Once modules\n",
    "try:\n",
    "    from opengait.modeling import models\n",
    "    from only_train_once import OTO\n",
    "    print(\"✓ Successfully imported OpenGait and OTO modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing required modules: {e}\")\n",
    "    print(\"Make sure you are running this notebook from the OpenGait directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b821c0",
   "metadata": {},
   "source": [
    "## Define Extraction Function\n",
    "\n",
    "The following cell defines the `extract_compressed_model` function that:\n",
    "1. Loads a GETA checkpoint\n",
    "2. Creates the model based on configuration\n",
    "3. Traces the model with dummy input\n",
    "4. Initializes OTO and constructs the compressed model\n",
    "5. Compares original and compressed models (parameters, size, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_compressed_model(checkpoint_path, output_dir='./compressed_models', \n",
    "                           model_name='GaitGLGeta', cfg_path='./configs/gaitgl/gaitgl_geta.yaml',\n",
    "                           visualize=True):\n",
    "    \"\"\"\n",
    "    Extract compressed model from a GETA checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        output_dir: Directory to save the compressed model\n",
    "        model_name: Name of the model class\n",
    "        cfg_path: Path to the model config file\n",
    "        visualize: Print model statistics comparison\n",
    "    \n",
    "    Returns:\n",
    "        Path to the compressed model if successful, None otherwise\n",
    "    \"\"\"\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    \n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        \n",
    "        # Get model configuration\n",
    "        from opengait.utils import config_loader\n",
    "        cfgs = config_loader(cfg_path)\n",
    "        \n",
    "        # Create model instance\n",
    "        print(f\"Creating {model_name} instance\")\n",
    "        ModelClass = getattr(models, model_name)\n",
    "        model = ModelClass(cfgs, training=False)\n",
    "        \n",
    "        # Load weights\n",
    "        if 'model' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            \n",
    "        # Move model to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create dummy input for the model\n",
    "        print(\"Creating dummy input for model tracing\")\n",
    "        batch_size = 4\n",
    "        seq_len = 30  # Frames per sequence\n",
    "        height = 64   # Height of silhouette\n",
    "        width = 44    # Width of silhouette\n",
    "        \n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        sils = torch.rand(batch_size, seq_len, 1, height, width).to(device)\n",
    "        labs = torch.zeros(batch_size).long().to(device)\n",
    "        typs = torch.zeros(batch_size).long().to(device)\n",
    "        vies = torch.zeros(batch_size).long().to(device)\n",
    "        seqL = torch.full((batch_size,), seq_len).long().to(device)\n",
    "        \n",
    "        dummy_input = [sils, labs, typs, vies, seqL]\n",
    "\n",
    "        # Create OTO instance and construct compressed model\n",
    "        print(\"Initializing OTO and constructing compressed model\")\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        model.oto = OTO(model=model, dummy_input=dummy_input)\n",
    "        compressed_model_path = model.construct_compressed_model(out_dir=output_dir)\n",
    "        \n",
    "        if compressed_model_path:\n",
    "            print(f\"✓ Compressed model saved to: {compressed_model_path}\")\n",
    "        else:\n",
    "            print(\"⚠ No compressed model was created, please check if GETA/HESSO was used during training\")\n",
    "            return None\n",
    "\n",
    "        # Compare models if visualize flag is set\n",
    "        if visualize and compressed_model_path:\n",
    "            # Load the compressed model\n",
    "            compressed_model = torch.load(compressed_model_path)\n",
    "            \n",
    "            # Count parameters\n",
    "            original_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            compressed_params = sum(p.numel() for p in compressed_model.parameters() if p.requires_grad)\n",
    "            \n",
    "            print(\"\\n======= MODEL STATISTICS =======\")\n",
    "            print(f\"Original model parameters: {original_params:,}\")\n",
    "            print(f\"Compressed model parameters: {compressed_params:,}\")\n",
    "            print(f\"Compression ratio: {compressed_params/original_params:.4f}\")\n",
    "            print(f\"Parameter reduction: {(1-compressed_params/original_params)*100:.2f}%\")\n",
    "            \n",
    "            # Calculate file sizes\n",
    "            checkpoint_size_mb = os.path.getsize(checkpoint_path) / (1024*1024)\n",
    "            compressed_size_mb = os.path.getsize(compressed_model_path) / (1024*1024)\n",
    "            \n",
    "            print(f\"\\nOriginal checkpoint size: {checkpoint_size_mb:.2f} MB\")\n",
    "            print(f\"Compressed model size: {compressed_size_mb:.2f} MB\")\n",
    "            print(f\"File size reduction: {(1-compressed_size_mb/checkpoint_size_mb)*100:.2f}%\")\n",
    "            \n",
    "            # Try to compute MACs/FLOPs if supported by OTO\n",
    "            try:\n",
    "                oto_original = OTO(model=model, dummy_input=dummy_input)\n",
    "                oto_compressed = OTO(model=compressed_model, dummy_input=dummy_input)\n",
    "                \n",
    "                # Calculate MACs (Multiply-Accumulate Operations)\n",
    "                original_macs = oto_original.compute_macs(in_million=True)['total']\n",
    "                compressed_macs = oto_compressed.compute_macs(in_million=True)['total']\n",
    "                \n",
    "                print(f\"\\nOriginal model MACs: {original_macs:.2f}M\")\n",
    "                print(f\"Compressed model MACs: {compressed_macs:.2f}M\")\n",
    "                print(f\"MACs reduction: {(1-compressed_macs/original_macs)*100:.2f}%\")\n",
    "                print(\"==================================\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not compute MACs: {e}\")\n",
    "        \n",
    "        return compressed_model_path\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting compressed model: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353473c8",
   "metadata": {},
   "source": [
    "## Parse Command Line Arguments\n",
    "\n",
    "This notebook can be run directly from the Kaggle terminal using the `papermill` library. It accepts command-line arguments through notebook parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default notebook parameters (can be overridden with papermill)\n",
    "checkpoint_path = None  # Path to the checkpoint file\n",
    "output_dir = \"./compressed_models\"\n",
    "model_name = \"GaitGLGeta\"\n",
    "cfg_path = \"./configs/gaitgl/gaitgl_geta.yaml\"\n",
    "visualize = True\n",
    "\n",
    "# If running directly (not with papermill), allow parsing from command line\n",
    "if 'get_ipython' in globals():\n",
    "    try:\n",
    "        import IPython\n",
    "        # Check if run through papermill (which sets the notebook parameters)\n",
    "        if IPython.get_ipython().user_ns.get('__PAPERMILL__') is not True:\n",
    "            # If not running through papermill, try to get arguments from sys.argv\n",
    "            import argparse\n",
    "            parser = argparse.ArgumentParser(description='Extract compressed model from GETA checkpoint')\n",
    "            parser.add_argument('--checkpoint', required=True, help='Path to the checkpoint file')\n",
    "            parser.add_argument('--output-dir', default='./compressed_models', help='Output directory for compressed model')\n",
    "            parser.add_argument('--model-name', default='GaitGLGeta', help='Model class name')\n",
    "            parser.add_argument('--cfg-path', default='./configs/gaitgl/gaitgl_geta.yaml', help='Path to config file')\n",
    "            parser.add_argument('--visualize', action='store_true', help='Print model statistics')\n",
    "\n",
    "            # Parse only known arguments (allows notebook to work with additional arguments from jupyter)\n",
    "            args, _ = parser.parse_known_args()\n",
    "            \n",
    "            # Update variables with command line arguments\n",
    "            if args.checkpoint:\n",
    "                checkpoint_path = args.checkpoint\n",
    "            if args.output_dir:\n",
    "                output_dir = args.output_dir\n",
    "            if args.model_name:\n",
    "                model_name = args.model_name\n",
    "            if args.cfg_path:\n",
    "                cfg_path = args.cfg_path\n",
    "            if args.visualize:\n",
    "                visualize = args.visualize\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse command line arguments: {e}\")\n",
    "\n",
    "# If no checkpoint path is provided, show a meaningful error\n",
    "if not checkpoint_path:\n",
    "    print(\"⚠️ Error: No checkpoint path provided!\")\n",
    "    print(\"Please provide a checkpoint path using: --checkpoint PATH_TO_CHECKPOINT\")\n",
    "    print(\"Example: python -m papermill extract_compressed.ipynb output.ipynb -p checkpoint_path /path/to/checkpoint.pt\")\n",
    "\n",
    "print(f\"Checkpoint path: {checkpoint_path}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Config path: {cfg_path}\")\n",
    "print(f\"Visualize: {visualize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ed992",
   "metadata": {},
   "source": [
    "## Extract Compressed Model\n",
    "\n",
    "Now let's run the extraction function with the provided parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f456f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if checkpoint path is provided\n",
    "if checkpoint_path:\n",
    "    try:\n",
    "        # Run extraction function\n",
    "        compressed_model_path = extract_compressed_model(\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            output_dir=output_dir,\n",
    "            model_name=model_name,\n",
    "            cfg_path=cfg_path,\n",
    "            visualize=visualize\n",
    "        )\n",
    "        \n",
    "        if compressed_model_path:\n",
    "            print(f\"\\n✅ Success! Compressed model extracted to: {compressed_model_path}\")\n",
    "        else:\n",
    "            print(\"\\n❌ Failed to extract compressed model.\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"\\n❌ Error during model extraction: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping extraction because no checkpoint path was provided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadfc69a",
   "metadata": {},
   "source": [
    "## How to Run\n",
    "\n",
    "This notebook can be run in different ways:\n",
    "\n",
    "### 1. From Jupyter notebook interface\n",
    "Simply fill in the `checkpoint_path` and other parameters in the \"Parse Command Line Arguments\" cell and run all cells.\n",
    "\n",
    "### 2. From terminal using papermill (recommended for Kaggle)\n",
    "```bash\n",
    "papermill extract_compressed.ipynb output.ipynb \\\n",
    "  -p checkpoint_path \"/path/to/checkpoint.pt\" \\\n",
    "  -p output_dir \"/output/directory\" \\\n",
    "  -p model_name \"GaitGLGeta\" \\\n",
    "  -p cfg_path \"/path/to/config.yaml\" \\\n",
    "  -p visualize True\n",
    "```\n",
    "\n",
    "### 3. From terminal using parameters with Python\n",
    "```bash\n",
    "python -c \"import sys; sys.path.append('/kaggle/working/geta_gaitGL/OpenGait'); \\\n",
    "  from extract_compressed import extract_compressed_model; \\\n",
    "  extract_compressed_model('/path/to/checkpoint.pt', '/output/directory', 'GaitGLGeta', '/path/to/config.yaml', True)\"\n",
    "```\n",
    "\n",
    "### Example for Kaggle\n",
    "\n",
    "```bash\n",
    "cd /kaggle/working/geta_gaitGL/OpenGait\n",
    "papermill extract_compressed.ipynb output.ipynb \\\n",
    "  -p checkpoint_path \"/kaggle/working/geta_gaitGL/OpenGait/output/CASIA-B/GaitGLGeta/GaitGL_GETA/checkpoints/GaitGL_GETA-80000.pt\" \\\n",
    "  -p output_dir \"/kaggle/working/compressed_models\" \\\n",
    "  -p visualize True\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
