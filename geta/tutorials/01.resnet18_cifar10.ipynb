{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4441533-4d5a-4c9e-8906-1df24bda96ec",
   "metadata": {},
   "source": [
    "## Tutorial 1. ResNet18 on CIFAR10. \n",
    "\n",
    "\n",
    "In this tutorial, we will show \n",
    "\n",
    "- How to end-to-end train and compress a ResNet18 from scratch on CIFAR10 to get a compressed ResNet18.\n",
    "- The compressed ResNet18 achives both **high performance** and **significant FLOPs and parameters reductions** than the full model. \n",
    "- The compressed ResNet18 **reduces about 92% parameters** to achieve **92.91% accuracy** only lower than the baseline by **0.11%**.\n",
    "- More detailed new HESSO optimizer setup. (Technical report regarding HESSO will be released on the early of 2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067c3a6-bf56-4c85-96fb-2d4da488680b",
   "metadata": {},
   "source": [
    "### Step 1. Create OTO instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ce0399-51c7-4afc-b0f0-35bda3698825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyi/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTO graph constructor\n",
      "graph build\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from sanity_check.backends.resnet_cifar10 import resnet18_cifar10\n",
    "from only_train_once import OTO\n",
    "import torch\n",
    "\n",
    "model = resnet18_cifar10()\n",
    "dummy_input = torch.rand(1, 3, 32, 32)\n",
    "oto = OTO(model=model.cuda(), dummy_input=dummy_input.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01639056-422d-4bfb-89db-ff9f0725e807",
   "metadata": {},
   "source": [
    "#### (Optional) Visualize the pruning dependancy graph of DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8cefb1-85cf-4833-a6ff-ee2f9270f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A ResNet_zig.gv.pdf will be generated to display the depandancy graph.\n",
    "oto.visualize(view=False, out_dir='../cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99fc48-fed7-41a5-ab6d-f2e5686f2003",
   "metadata": {},
   "source": [
    "### Step 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ec145d-4847-4f4b-8c12-782beb61cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trainset = CIFAR10(root='cifar10', train=True, download=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "testset = CIFAR10(root='cifar10', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "trainloader =  torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65233349-da36-47bc-9af8-41a3f2d4429a",
   "metadata": {},
   "source": [
    "### Step 3. Setup HESSO optimizer\n",
    "\n",
    "The following main hyperparameters need to be taken care.\n",
    "\n",
    "- `variant`: The optimizer that is used for training the baseline full model. Currently support `sgd`, `adam` and `adamw`.\n",
    "- `lr`: The initial learning rate.\n",
    "- `weight_decay`: Weight decay as standard DNN optimization.\n",
    "- `target_group_sparsity`: The target group sparsity, typically higher group sparsity refers to more FLOPs and model size reduction, meanwhile may regress model performance more.\n",
    "- `start_pruning_steps`: The number of steps that **starts** to prune.\n",
    "- `pruning_steps`: The number of steps that **finishes** pruning (reach `target_group_sparsity`) after `start_pruning_steps`.\n",
    "- `pruning_periods`:  Incrementally produce the group sparsity equally among pruning periods.\n",
    "\n",
    "We empirically suggest `start_pruning_steps` as 1/10 of total number of training steps. `pruning_steps` until 1/4 or 1/5 of total number of training steps.\n",
    "The advatnages of HESSO compared to DHSPG is its explicit control over group sparsity exploration, which is typically more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48af4116-bb7e-4156-bc3d-c32c8fc6f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup HESSO\n",
      "Target redundant groups per period:  [201, 201, 201, 201, 201, 201, 201, 201, 201, 206]\n"
     ]
    }
   ],
   "source": [
    "optimizer = oto.hesso(\n",
    "    variant='sgd', \n",
    "    lr=0.1, \n",
    "    weight_decay=1e-4,\n",
    "    target_group_sparsity=0.7,\n",
    "    start_pruning_step=10 * len(trainloader), \n",
    "    pruning_periods=10,\n",
    "    pruning_steps=10 * len(trainloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632cb7e-e1ae-460c-b415-81c158c80a20",
   "metadata": {},
   "source": [
    "### Step 4. Train ResNet18 as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9d98a1-a77b-4cc6-9acd-a1483c17e370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyi/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyi/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712609048481/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, loss: 1.61, norm_all:4123.36, grp_sparsity: 0.00, acc1: 0.3796, norm_import: 4123.36, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 1, loss: 1.06, norm_all:4114.36, grp_sparsity: 0.00, acc1: 0.4631, norm_import: 4114.36, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 2, loss: 0.81, norm_all:4105.85, grp_sparsity: 0.00, acc1: 0.7201, norm_import: 4105.85, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 3, loss: 0.66, norm_all:4095.98, grp_sparsity: 0.00, acc1: 0.7359, norm_import: 4095.98, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 4, loss: 0.57, norm_all:4085.42, grp_sparsity: 0.00, acc1: 0.7351, norm_import: 4085.42, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 5, loss: 0.50, norm_all:4074.24, grp_sparsity: 0.00, acc1: 0.7689, norm_import: 4074.24, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 6, loss: 0.45, norm_all:4062.65, grp_sparsity: 0.00, acc1: 0.8144, norm_import: 4062.65, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 7, loss: 0.41, norm_all:4050.68, grp_sparsity: 0.00, acc1: 0.7995, norm_import: 4050.68, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 8, loss: 0.38, norm_all:4038.44, grp_sparsity: 0.00, acc1: 0.8003, norm_import: 4038.44, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "Ep: 9, loss: 0.35, norm_all:4026.47, grp_sparsity: 0.00, acc1: 0.6614, norm_import: 4026.47, norm_redund: 0.00, num_grp_import: 2880, num_grp_redund: 0\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 10, loss: 0.33, norm_all:3796.72, grp_sparsity: 0.07, acc1: 0.6809, norm_import: 3796.72, norm_redund: 0.00, num_grp_import: 2679, num_grp_redund: 201\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 11, loss: 0.32, norm_all:3569.59, grp_sparsity: 0.14, acc1: 0.8455, norm_import: 3569.59, norm_redund: 0.00, num_grp_import: 2478, num_grp_redund: 402\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 12, loss: 0.31, norm_all:3344.81, grp_sparsity: 0.21, acc1: 0.6907, norm_import: 3344.81, norm_redund: 0.00, num_grp_import: 2277, num_grp_redund: 603\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 13, loss: 0.30, norm_all:3119.70, grp_sparsity: 0.28, acc1: 0.8680, norm_import: 3119.70, norm_redund: 0.00, num_grp_import: 2076, num_grp_redund: 804\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 14, loss: 0.29, norm_all:2856.03, grp_sparsity: 0.35, acc1: 0.8653, norm_import: 2856.03, norm_redund: 0.00, num_grp_import: 1875, num_grp_redund: 1005\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 15, loss: 0.28, norm_all:2558.15, grp_sparsity: 0.42, acc1: 0.7711, norm_import: 2558.15, norm_redund: 0.00, num_grp_import: 1674, num_grp_redund: 1206\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 16, loss: 0.29, norm_all:2243.45, grp_sparsity: 0.49, acc1: 0.7854, norm_import: 2243.45, norm_redund: 0.00, num_grp_import: 1473, num_grp_redund: 1407\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 17, loss: 0.29, norm_all:1947.95, grp_sparsity: 0.56, acc1: 0.7680, norm_import: 1947.95, norm_redund: 0.00, num_grp_import: 1272, num_grp_redund: 1608\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 18, loss: 0.31, norm_all:1655.40, grp_sparsity: 0.63, acc1: 0.7204, norm_import: 1655.40, norm_redund: 0.00, num_grp_import: 1071, num_grp_redund: 1809\n",
      "0 ['conv1.1.weight', 'conv1.1.bias', 'conv2_x.1.residual_function.4.weight', 'conv2_x.1.residual_function.4.bias', 'conv2_x.0.residual_function.4.weight', 'conv2_x.0.residual_function.4.bias', 'conv1.0.weight', 'conv2_x.0.residual_function.3.weight', 'conv2_x.1.residual_function.3.weight'] torch.Size([64])\n",
      "1 ['conv2_x.0.residual_function.1.weight', 'conv2_x.0.residual_function.1.bias', 'conv2_x.0.residual_function.0.weight'] torch.Size([64])\n",
      "2 ['conv2_x.1.residual_function.1.weight', 'conv2_x.1.residual_function.1.bias', 'conv2_x.1.residual_function.0.weight'] torch.Size([64])\n",
      "3 ['conv3_x.0.residual_function.1.weight', 'conv3_x.0.residual_function.1.bias', 'conv3_x.0.residual_function.0.weight'] torch.Size([128])\n",
      "4 ['conv3_x.0.residual_function.4.weight', 'conv3_x.0.residual_function.4.bias', 'conv3_x.1.residual_function.4.weight', 'conv3_x.1.residual_function.4.bias', 'conv3_x.0.shortcut.1.weight', 'conv3_x.0.shortcut.1.bias', 'conv3_x.0.residual_function.3.weight', 'conv3_x.0.shortcut.0.weight', 'conv3_x.1.residual_function.3.weight'] torch.Size([128])\n",
      "5 ['conv3_x.1.residual_function.1.weight', 'conv3_x.1.residual_function.1.bias', 'conv3_x.1.residual_function.0.weight'] torch.Size([128])\n",
      "6 ['conv4_x.0.residual_function.1.weight', 'conv4_x.0.residual_function.1.bias', 'conv4_x.0.residual_function.0.weight'] torch.Size([256])\n",
      "7 ['conv4_x.0.residual_function.4.weight', 'conv4_x.0.residual_function.4.bias', 'conv4_x.1.residual_function.4.weight', 'conv4_x.1.residual_function.4.bias', 'conv4_x.0.shortcut.1.weight', 'conv4_x.0.shortcut.1.bias', 'conv4_x.0.residual_function.3.weight', 'conv4_x.0.shortcut.0.weight', 'conv4_x.1.residual_function.3.weight'] torch.Size([256])\n",
      "8 ['conv4_x.1.residual_function.1.weight', 'conv4_x.1.residual_function.1.bias', 'conv4_x.1.residual_function.0.weight'] torch.Size([256])\n",
      "9 ['conv5_x.0.residual_function.1.weight', 'conv5_x.0.residual_function.1.bias', 'conv5_x.0.residual_function.0.weight'] torch.Size([512])\n",
      "10 ['conv5_x.0.residual_function.4.weight', 'conv5_x.0.residual_function.4.bias', 'conv5_x.1.residual_function.4.weight', 'conv5_x.1.residual_function.4.bias', 'conv5_x.0.shortcut.1.weight', 'conv5_x.0.shortcut.1.bias', 'conv5_x.0.residual_function.3.weight', 'conv5_x.0.shortcut.0.weight', 'conv5_x.1.residual_function.3.weight'] torch.Size([512])\n",
      "11 ['conv5_x.1.residual_function.1.weight', 'conv5_x.1.residual_function.1.bias', 'conv5_x.1.residual_function.0.weight'] torch.Size([512])\n",
      "Ep: 19, loss: 0.35, norm_all:1354.99, grp_sparsity: 0.70, acc1: 0.7569, norm_import: 1354.99, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 20, loss: 0.35, norm_all:1367.94, grp_sparsity: 0.70, acc1: 0.7147, norm_import: 1367.94, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 21, loss: 0.31, norm_all:1377.83, grp_sparsity: 0.70, acc1: 0.6945, norm_import: 1377.83, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 22, loss: 0.28, norm_all:1385.51, grp_sparsity: 0.70, acc1: 0.7299, norm_import: 1385.51, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 23, loss: 0.26, norm_all:1392.40, grp_sparsity: 0.70, acc1: 0.7639, norm_import: 1392.40, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 24, loss: 0.24, norm_all:1398.09, grp_sparsity: 0.70, acc1: 0.8783, norm_import: 1398.09, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 25, loss: 0.23, norm_all:1403.53, grp_sparsity: 0.70, acc1: 0.8547, norm_import: 1403.53, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 26, loss: 0.21, norm_all:1407.87, grp_sparsity: 0.70, acc1: 0.8789, norm_import: 1407.87, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 27, loss: 0.21, norm_all:1412.72, grp_sparsity: 0.70, acc1: 0.8425, norm_import: 1412.72, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 28, loss: 0.20, norm_all:1416.83, grp_sparsity: 0.70, acc1: 0.8697, norm_import: 1416.83, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 29, loss: 0.18, norm_all:1420.66, grp_sparsity: 0.70, acc1: 0.8381, norm_import: 1420.66, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 30, loss: 0.18, norm_all:1424.24, grp_sparsity: 0.70, acc1: 0.8628, norm_import: 1424.24, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 31, loss: 0.17, norm_all:1428.15, grp_sparsity: 0.70, acc1: 0.8006, norm_import: 1428.15, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 32, loss: 0.16, norm_all:1430.97, grp_sparsity: 0.70, acc1: 0.8981, norm_import: 1430.97, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 33, loss: 0.16, norm_all:1433.85, grp_sparsity: 0.70, acc1: 0.8951, norm_import: 1433.85, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 34, loss: 0.15, norm_all:1436.90, grp_sparsity: 0.70, acc1: 0.7830, norm_import: 1436.90, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 35, loss: 0.15, norm_all:1439.75, grp_sparsity: 0.70, acc1: 0.8683, norm_import: 1439.75, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 36, loss: 0.14, norm_all:1442.19, grp_sparsity: 0.70, acc1: 0.8797, norm_import: 1442.19, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 37, loss: 0.14, norm_all:1444.35, grp_sparsity: 0.70, acc1: 0.8967, norm_import: 1444.35, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 38, loss: 0.13, norm_all:1446.39, grp_sparsity: 0.70, acc1: 0.8897, norm_import: 1446.39, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 39, loss: 0.13, norm_all:1448.35, grp_sparsity: 0.70, acc1: 0.9028, norm_import: 1448.35, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 40, loss: 0.12, norm_all:1449.85, grp_sparsity: 0.70, acc1: 0.8793, norm_import: 1449.85, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 41, loss: 0.12, norm_all:1451.24, grp_sparsity: 0.70, acc1: 0.9075, norm_import: 1451.24, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 42, loss: 0.12, norm_all:1453.18, grp_sparsity: 0.70, acc1: 0.8808, norm_import: 1453.18, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 43, loss: 0.11, norm_all:1454.46, grp_sparsity: 0.70, acc1: 0.8725, norm_import: 1454.46, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 44, loss: 0.11, norm_all:1456.40, grp_sparsity: 0.70, acc1: 0.8624, norm_import: 1456.40, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 45, loss: 0.11, norm_all:1457.83, grp_sparsity: 0.70, acc1: 0.8853, norm_import: 1457.83, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 46, loss: 0.10, norm_all:1458.56, grp_sparsity: 0.70, acc1: 0.9010, norm_import: 1458.56, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 47, loss: 0.10, norm_all:1459.77, grp_sparsity: 0.70, acc1: 0.8291, norm_import: 1459.77, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 48, loss: 0.10, norm_all:1461.19, grp_sparsity: 0.70, acc1: 0.8751, norm_import: 1461.19, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 49, loss: 0.06, norm_all:1460.21, grp_sparsity: 0.70, acc1: 0.9235, norm_import: 1460.21, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 50, loss: 0.05, norm_all:1459.23, grp_sparsity: 0.70, acc1: 0.9243, norm_import: 1459.23, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 51, loss: 0.04, norm_all:1458.25, grp_sparsity: 0.70, acc1: 0.9247, norm_import: 1458.25, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 52, loss: 0.04, norm_all:1457.26, grp_sparsity: 0.70, acc1: 0.9254, norm_import: 1457.26, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 53, loss: 0.03, norm_all:1456.27, grp_sparsity: 0.70, acc1: 0.9264, norm_import: 1456.27, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 54, loss: 0.03, norm_all:1455.27, grp_sparsity: 0.70, acc1: 0.9269, norm_import: 1455.27, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 55, loss: 0.03, norm_all:1454.26, grp_sparsity: 0.70, acc1: 0.9270, norm_import: 1454.26, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 56, loss: 0.03, norm_all:1453.26, grp_sparsity: 0.70, acc1: 0.9269, norm_import: 1453.26, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 57, loss: 0.03, norm_all:1452.25, grp_sparsity: 0.70, acc1: 0.9268, norm_import: 1452.25, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 58, loss: 0.03, norm_all:1451.23, grp_sparsity: 0.70, acc1: 0.9256, norm_import: 1451.23, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 59, loss: 0.03, norm_all:1450.22, grp_sparsity: 0.70, acc1: 0.9247, norm_import: 1450.22, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 60, loss: 0.02, norm_all:1449.21, grp_sparsity: 0.70, acc1: 0.9285, norm_import: 1449.21, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 61, loss: 0.02, norm_all:1448.19, grp_sparsity: 0.70, acc1: 0.9281, norm_import: 1448.19, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 62, loss: 0.02, norm_all:1447.18, grp_sparsity: 0.70, acc1: 0.9244, norm_import: 1447.18, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 63, loss: 0.02, norm_all:1446.16, grp_sparsity: 0.70, acc1: 0.9241, norm_import: 1446.16, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 64, loss: 0.02, norm_all:1445.13, grp_sparsity: 0.70, acc1: 0.9260, norm_import: 1445.13, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 65, loss: 0.02, norm_all:1444.11, grp_sparsity: 0.70, acc1: 0.9271, norm_import: 1444.11, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 66, loss: 0.02, norm_all:1443.09, grp_sparsity: 0.70, acc1: 0.9256, norm_import: 1443.09, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 67, loss: 0.02, norm_all:1442.07, grp_sparsity: 0.70, acc1: 0.9278, norm_import: 1442.07, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 68, loss: 0.02, norm_all:1441.04, grp_sparsity: 0.70, acc1: 0.9268, norm_import: 1441.04, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 69, loss: 0.02, norm_all:1440.02, grp_sparsity: 0.70, acc1: 0.9270, norm_import: 1440.02, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 70, loss: 0.02, norm_all:1438.99, grp_sparsity: 0.70, acc1: 0.9272, norm_import: 1438.99, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 71, loss: 0.02, norm_all:1437.97, grp_sparsity: 0.70, acc1: 0.9253, norm_import: 1437.97, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 72, loss: 0.02, norm_all:1436.94, grp_sparsity: 0.70, acc1: 0.9258, norm_import: 1436.94, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 73, loss: 0.02, norm_all:1435.92, grp_sparsity: 0.70, acc1: 0.9281, norm_import: 1435.92, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 74, loss: 0.02, norm_all:1434.89, grp_sparsity: 0.70, acc1: 0.9263, norm_import: 1434.89, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 75, loss: 0.02, norm_all:1433.87, grp_sparsity: 0.70, acc1: 0.9291, norm_import: 1433.87, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 76, loss: 0.01, norm_all:1432.84, grp_sparsity: 0.70, acc1: 0.9272, norm_import: 1432.84, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 77, loss: 0.01, norm_all:1431.81, grp_sparsity: 0.70, acc1: 0.9269, norm_import: 1431.81, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 78, loss: 0.01, norm_all:1430.79, grp_sparsity: 0.70, acc1: 0.9259, norm_import: 1430.79, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 79, loss: 0.01, norm_all:1429.76, grp_sparsity: 0.70, acc1: 0.9267, norm_import: 1429.76, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 80, loss: 0.01, norm_all:1428.72, grp_sparsity: 0.70, acc1: 0.9278, norm_import: 1428.72, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 81, loss: 0.01, norm_all:1427.69, grp_sparsity: 0.70, acc1: 0.9264, norm_import: 1427.69, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 82, loss: 0.01, norm_all:1426.67, grp_sparsity: 0.70, acc1: 0.9262, norm_import: 1426.67, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 83, loss: 0.01, norm_all:1425.64, grp_sparsity: 0.70, acc1: 0.9256, norm_import: 1425.64, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 84, loss: 0.01, norm_all:1424.61, grp_sparsity: 0.70, acc1: 0.9262, norm_import: 1424.61, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 85, loss: 0.01, norm_all:1423.59, grp_sparsity: 0.70, acc1: 0.9245, norm_import: 1423.59, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 86, loss: 0.01, norm_all:1422.56, grp_sparsity: 0.70, acc1: 0.9265, norm_import: 1422.56, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 87, loss: 0.01, norm_all:1421.53, grp_sparsity: 0.70, acc1: 0.9250, norm_import: 1421.53, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 88, loss: 0.01, norm_all:1420.50, grp_sparsity: 0.70, acc1: 0.9262, norm_import: 1420.50, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 89, loss: 0.01, norm_all:1419.48, grp_sparsity: 0.70, acc1: 0.9271, norm_import: 1419.48, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 90, loss: 0.01, norm_all:1418.45, grp_sparsity: 0.70, acc1: 0.9264, norm_import: 1418.45, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 91, loss: 0.01, norm_all:1417.42, grp_sparsity: 0.70, acc1: 0.9251, norm_import: 1417.42, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 92, loss: 0.01, norm_all:1416.39, grp_sparsity: 0.70, acc1: 0.9269, norm_import: 1416.39, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 93, loss: 0.01, norm_all:1415.36, grp_sparsity: 0.70, acc1: 0.9260, norm_import: 1415.36, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 94, loss: 0.01, norm_all:1414.33, grp_sparsity: 0.70, acc1: 0.9270, norm_import: 1414.33, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 95, loss: 0.01, norm_all:1413.30, grp_sparsity: 0.70, acc1: 0.9273, norm_import: 1413.30, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 96, loss: 0.01, norm_all:1412.28, grp_sparsity: 0.70, acc1: 0.9283, norm_import: 1412.28, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 97, loss: 0.01, norm_all:1411.25, grp_sparsity: 0.70, acc1: 0.9257, norm_import: 1411.25, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 98, loss: 0.01, norm_all:1410.22, grp_sparsity: 0.70, acc1: 0.9263, norm_import: 1410.22, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n",
      "Ep: 99, loss: 0.01, norm_all:1410.12, grp_sparsity: 0.70, acc1: 0.9269, norm_import: 1410.12, norm_redund: 0.00, num_grp_import: 865, num_grp_redund: 2015\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import check_accuracy\n",
    "\n",
    "max_epoch = 100\n",
    "model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Every 50 epochs, decay lr by 10.0\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) \n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    f_avg_val = 0.0\n",
    "    model.train()\n",
    "    lr_scheduler.step()\n",
    "    for X, y in trainloader:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        y_pred = model.forward(X)\n",
    "        f = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        f.backward()\n",
    "        f_avg_val += f\n",
    "        optimizer.step()\n",
    "    opt_metrics = optimizer.compute_metrics()\n",
    "    # group_sparsity, param_norm, _ = optimizer.compute_group_sparsity_param_norm()\n",
    "    # norm_important, norm_redundant, num_grps_important, num_grps_redundant = optimizer.compute_norm_groups()\n",
    "    accuracy1, accuracy5 = check_accuracy(model, testloader)\n",
    "    f_avg_val = f_avg_val.cpu().item() / len(trainloader)\n",
    "    \n",
    "    print(\"Ep: {ep}, loss: {f:.2f}, norm_all:{param_norm:.2f}, grp_sparsity: {gs:.2f}, acc1: {acc1:.4f}, norm_import: {norm_import:.2f}, norm_redund: {norm_redund:.2f}, num_grp_import: {num_grps_import}, num_grp_redund: {num_grps_redund}\"\\\n",
    "         .format(ep=epoch, f=f_avg_val, param_norm=opt_metrics.norm_params, gs=opt_metrics.group_sparsity, acc1=accuracy1,\\\n",
    "         norm_import=opt_metrics.norm_important_groups, norm_redund=opt_metrics.norm_redundant_groups, \\\n",
    "         num_grps_import=opt_metrics.num_important_groups, num_grps_redund=opt_metrics.num_redundant_groups\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dabf9-04cf-4d55-89e4-a17c96d17cec",
   "metadata": {},
   "source": [
    "### Step 5. Get compressed model in torch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46e6b6e-6286-4f57-816f-f94f6ce5c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default OTO will construct subnet by the last checkpoint. If intermedia ckpt reaches the best performance,\n",
    "# need to reinitialize OTO instance\n",
    "# oto = OTO(torch.load(ckpt_path), dummy_input)\n",
    "# then construct subnetwork\n",
    "oto.construct_subnet(out_dir='./cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9cd3b6-1db9-473d-9585-27e08c669265",
   "metadata": {},
   "source": [
    "### (Optional) Check the compressed model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc731c99-3155-472e-9462-65ed8c9bf4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of full model     :  0.041716959327459335 GBs\n",
      "Size of compress model :  0.003497043624520302 GBs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "full_model_size = os.stat(oto.full_group_sparse_model_path)\n",
    "compressed_model_size = os.stat(oto.compressed_model_path)\n",
    "print(\"Size of full model     : \", full_model_size.st_size / (1024 ** 3), \"GBs\")\n",
    "print(\"Size of compress model : \", compressed_model_size.st_size / (1024 ** 3), \"GBs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d825e-46c8-49e9-b9f0-6f4eea4d8e2d",
   "metadata": {},
   "source": [
    "### (Optional) Check the compressed model accuracy\n",
    "#### # Both full and compressed model should return the exact same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be56f693-3566-4b91-b5a7-39f327e61450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model: Acc 1: 0.9269, Acc 5: 0.9971\n",
      "Compressed model: Acc 1: 0.9269, Acc 5: 0.9971\n"
     ]
    }
   ],
   "source": [
    "full_model = torch.load(oto.full_group_sparse_model_path)\n",
    "compressed_model = torch.load(oto.compressed_model_path)\n",
    "\n",
    "acc1_full, acc5_full = check_accuracy(full_model, testloader)\n",
    "print(\"Full model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_full, acc5=acc5_full))\n",
    "\n",
    "acc1_compressed, acc5_compressed = check_accuracy(compressed_model, testloader)\n",
    "print(\"Compressed model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_compressed, acc5=acc5_compressed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
